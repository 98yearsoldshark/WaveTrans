# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
from peft import PeftModel
import json
import pandas as pd

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 ç¿»è¯‘åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')
# model_dir = snapshot_download('IEITYuan/Yuan2-2B-July-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'
lora_path = './output/Yuan2.0-2B_lora_bf16/checkpoint-135'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
    model = PeftModel.from_pretrained(model, model_id=lora_path)

    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

template = '''
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ AI ç¿»è¯‘åŠ©ç†ï¼Œå…·å¤‡å“è¶Šçš„è¯­è¨€èƒ½åŠ›ï¼Œèƒ½å¤Ÿç²¾å‡†ã€æµç•…ä¸”åœ°é“åœ°ç¿»è¯‘è‹±è¯­æ–‡æœ¬ã€‚

# æŠ€èƒ½
1. å½“ç”¨æˆ·æä¾›è‹±è¯­ï¼Œä½ è¾“å‡ºä¸­æ–‡

# å›å¤ç¤ºä¾‹
è¾“å…¥ï¼š Previously we obtained recombinant soluble human rhIFN-Î»1 from Pichia pastoris.
è¾“å‡º: æ­¤å‰ï¼Œæˆ‘ä»¬å·²ç»ä»æ¯•èµ¤é…µæ¯è¡¨è¾¾è·å¾—äº†å¯æº¶æ€§é‡ç»„äººå¹²æ‰°ç´ -Î»1ã€‚

# é™åˆ¶:
- ä»…ä¸“æ³¨äºç¿»è¯‘ç›¸å…³çš„ä»»åŠ¡ï¼Œåšå†³æ‹’ç»å›ç­”ä¸ç¿»è¯‘æ— å…³çš„ä»»ä½•é—®é¢˜ã€‚
- è¯‘æ–‡è¦æ¸…æ™°æ˜“æ‡‚ã€è‡ªç„¶æµç•…ï¼Œé¿å…ä½¿ç”¨ç”Ÿåƒ»æˆ–è¿‡äºå¤æ‚çš„ä¸­æ–‡è¡¨è¿°ã€‚

query

'''




# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if query := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": query})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(query)

    # è°ƒç”¨æ¨¡å‹
    prompt = template.replace('query', query).strip() + "<sep>" # æ‹¼æ¥å¯¹è¯å†å²
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)


